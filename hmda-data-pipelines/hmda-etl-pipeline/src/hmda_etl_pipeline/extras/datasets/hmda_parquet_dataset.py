"""``HMDAParquetDataset`` loads/saves data from/to a Parquet file using an underlying
filesystem (e.g.: local, S3, GCS). It uses pandas to handle the Parquet file.
"""

from datetime import datetime
from io import BytesIO
from pathlib import Path, PurePosixPath
import pytz
from typing import Any, Dict

import pandas as pd
from kedro.io.core import (
    Version,
    get_filepath_str,
)

from kedro.io import DatasetError
from kedro_datasets.pandas import ParquetDataset


class HMDAParquetDataset(ParquetDataset):
    # pylint: disable=too-many-arguments
    def __init__(
        self,
        filepath: str,
        load_args: Dict[str, Any] = None,
        save_args: Dict[str, Any] = None,
        version: Version = None,
        credentials: Dict[str, Any] = None,
        fs_args: Dict[str, Any] = None,
        metadata: Dict[str, Any] = None,
    ) -> None:
        """Creates a new instance of ``HMDAParquetDataset`` pointing to a concrete Parquet file
        on a specific filesystem. The HMDA version modifies the ParquetDataset to allow
        for saving to two locations: the latest directory and a directory named after today's date.

        Args:
            filepath: Filepath in POSIX format to a Parquet file prefixed with a protocol like
                `s3://`. If prefix is not provided, `file` protocol (local filesystem) will be used.
                The prefix should be any protocol supported by ``fsspec``.
                It can also be a path to a directory. If the directory is
                provided then it can be used for reading partitioned parquet files.
                Note: `http(s)` doesn't support versioning.
            load_args: Additional options for loading Parquet file(s).
                Here you can find all available arguments when reading single file:
                https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_parquet.html
                Here you can find all available arguments when reading partitioned datasets:
                https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetDataset.html#pyarrow.parquet.ParquetDataset.read
                All defaults are preserved.
            save_args: Additional saving options for saving Parquet file(s).
                Here you can find all available arguments:
                https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_parquet.html
                All defaults are preserved. ``partition_cols`` is not supported.
            version: If specified, should be an instance of ``kedro.io.core.Version``.
                If its ``load`` attribute is None, the latest version will be loaded. If
                its ``save`` attribute is None, save version will be autogenerated.
            credentials: Credentials required to get access to the underlying filesystem.
                E.g. for ``GCSFileSystem`` it should look like `{"token": None}`.
            fs_args: Extra arguments to pass into underlying filesystem class constructor
                (e.g. `{"project": "my-project"}` for ``GCSFileSystem``).
            metadata: Any arbitrary metadata.
                This is ignored by Kedro, but may be consumed by users or external plugins.
        """

        super().__init__(
            filepath=filepath,
            load_args=load_args,
            save_args=save_args,
            version=version,
            credentials=credentials,
            fs_args=fs_args,
            metadata=metadata,
        )

    def _describe(self) -> Dict[str, Any]:
        return {
            "filepath": self._filepath,
            "protocol": self._protocol,
            "load_args": self._load_args,
            "save_args": self._save_args,
            "version": self._version,
        }

    def _save(self, data: pd.DataFrame) -> None:
        latest_save_path = get_filepath_str(
            PurePosixPath(self._filepath), self._protocol
        )
        date = datetime.strftime(
            datetime.now(pytz.timezone("America/New_York")), r"%Y-%m-%d"
        )
        dated_save_path = latest_save_path.replace("latest", date)

        if Path(latest_save_path).is_dir() or Path(dated_save_path).is_dir():
            raise DatasetError(
                f"Saving {self.__class__.__name__} to a directory is not supported."
            )

        if "partition_cols" in self._save_args:
            raise DatasetError(
                f"{self.__class__.__name__} does not support save argument "
                f"'partition_cols'. Please use 'kedro.io.PartitionedDataset' instead."
            )

        bytes_buffer = BytesIO()
        data.to_parquet(bytes_buffer, **self._save_args)

        # Save to latest folder
        with self._fs.open(latest_save_path, mode="wb") as fs_file_latest:
            fs_file_latest.write(bytes_buffer.getvalue())

        # Save to dated folder
        with self._fs.open(dated_save_path, mode="wb") as fs_file_dated:
            fs_file_dated.write(bytes_buffer.getvalue())

        self._invalidate_cache()
